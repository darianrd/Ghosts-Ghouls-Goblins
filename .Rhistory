tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "f_meas")
# Finalize workflow and fit
final_workflow <- rf_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Make predictions
rf_preds <- final_workflow |>
predict(new_data = GGGtest, type = "class")
library(tidymodels)
library(vroom)
library(embed)
GGGtrain <- vroom("train.csv")
GGGtest <- vroom("test.csv")
# Create recipe
GGGrecipe <- recipe(type ~ ., data = GGGtrain) |>
step_mutate_at(all_nominal_predictors(), fn = factor) |>
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) |>
step_normalize(all_numeric_predictors())
# Create random forest model
GGG_rf <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) |>
set_engine("ranger") |>
set_mode("classification")
# Create workflow
rf_workflow <- workflow() |>
add_model(GGG_rf) |>
add_recipe(GGGrecipe)
# Grid of values to tune over
tuning_grid <- grid_regular(mtry(range=c(1,6)),
min_n(),
levels = 5)
# Split data for CV
folds <- vfold_cv(GGGtrain, v = 10, repeats = 1)
# Run CV
CV_results <- rf_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "f_meas")
# Finalize workflow and fit
final_workflow <- rf_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Make predictions
rf_preds <- final_workflow |>
predict(new_data = GGGtest, type = "class")
# Prep for Kaggle submission
kaggle_sub <- rf_preds %>%
bind_cols(., GGGtest) |> # Bind predictions to test data
rename(type = .pred_class) |> # Rename .pred_1 to ACTION for Kaggle submission
select(id, type) # Keep id and ACTION variables
# Write out file
vroom_write(x = kaggle_sub, file = "./RFPreds.csv", delim = ",")
GGGtrain$id <- factor(GGGtrain$id)
# Create recipe
GGGrecipe <- recipe(type ~ ., data = GGGtrain) |>
step_mutate_at(all_nominal_predictors(), fn = factor) |>
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) |>
step_normalize(all_numeric_predictors())
# Create random forest model
GGG_rf <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) |>
set_engine("ranger") |>
set_mode("classification")
# Create workflow
rf_workflow <- workflow() |>
add_model(GGG_rf) |>
add_recipe(GGGrecipe)
# Grid of values to tune over
tuning_grid <- grid_regular(mtry(range=c(1,6)),
min_n(),
levels = 5)
# Split data for CV
folds <- vfold_cv(GGGtrain, v = 10, repeats = 1)
# Run CV
CV_results <- rf_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
library(tidymodels)
library(vroom)
GGGtrain <- vroom("train.csv")
GGGtest <- vroom("test.csv")
View(GGGtrain)
library(tidymodels)
library(vroom)
# Read in data
GGGtrain <- vroom("train.csv")
GGGtest <- vroom("test.csv")
# Create recipe
GGG_recipe <- recipe(type ~ ., data = GGGtrain) |>
update_role(id, new_role = "id") |>
step_mutate_at(all_nominal_predictors(), fn = factor) |>
step_dummy(all_nominal_predictors()) |>
step_range(all_numeric_predictors(), min = 0, max = 1)
# Create neural network model
nn_mod <- mlp(hidden_units = tune(),
epochs = 100) |>
set_engine("keras") |>
set_mode("classification")
# Create workflow
nn_workflow <- workflow() |>
add_model(nn_mod) |>
add_recipe(GGG_recipe)
# Grid of values to tune over
tuning_grid <- grid_regular(hidden_units(rance = c(1, 20)),
levels = 5)
# Grid of values to tune over
tuning_grid <- grid_regular(hidden_units(range = c(1, 20)),
levels = 5)
# Tune model
tuned_nn <- nn_workflow |>
tune_grid(tuning_grid)
# Tune model
tuned_nn <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision)))
# Tune model
tuned_nn <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
# Split data for CV
folds <- vfold_cv(GGGtrain, v = 10, repeats = 1)
# Tune model
tuned_nn <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
# Grid of values to tune over
tuning_grid <- grid_regular(hidden_units(range = c(1, 20)),
levels = 5)
# Split data for CV
folds <- vfold_cv(GGGtrain, v = 10, repeats = 1)
# Run CV
CV_results <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
library(tidymodels)
library(vroom)
# Read in data
GGGtrain <- vroom("train.csv")
GGGtest <- vroom("test.csv")
# Create recipe
GGG_recipe <- recipe(type ~ ., data = GGGtrain) |>
update_role(id, new_role = "id") |>
step_mutate_at(all_nominal_predictors(), fn = factor) |>
step_dummy(all_nominal_predictors()) |>
step_range(all_numeric_predictors(), min = 0, max = 1)
# Create neural network model
nn_mod <- mlp(hidden_units = tune(),
epochs = 100) |>
set_engine("keras") |>
set_mode("classification")
# Create workflow
nn_workflow <- workflow() |>
add_model(nn_mod) |>
add_recipe(GGG_recipe)
# Grid of values to tune over
tuning_grid <- grid_regular(hidden_units(range = c(1, 20)),
levels = 5)
# Split data for CV
folds <- vfold_cv(GGGtrain, v = 10, repeats = 1)
# Run CV
CV_results <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "accuracy")
# Finalize workflow and fit
final_workflow <- nn_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Make predictions
nn_preds <- final_workflow |>
predict(new_data = GGGtest, type = "class")
View(nn_preds)
View(nn_preds)
# Create recipe
GGG_recipe <- recipe(type ~ ., data = GGGtrain) |>
update_role(id, new_role = "id") |>
step_mutate_at(color, fn = factor) |>
step_dummy(color) |>
step_range(all_numeric_predictors(), min = 0, max = 1)
# Create neural network model
nn_mod <- mlp(hidden_units = tune(),
epochs = 100) |>
set_engine("keras") |>
set_mode("classification")
# Create workflow
nn_workflow <- workflow() |>
add_model(nn_mod) |>
add_recipe(GGG_recipe)
# Grid of values to tune over
tuning_grid <- grid_regular(hidden_units(range = c(1, 20)),
levels = 5)
# Split data for CV
folds <- vfold_cv(GGGtrain, v = 10, repeats = 1)
# Run CV
CV_results <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "accuracy")
# Finalize workflow and fit
final_workflow <- nn_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Make predictions
nn_preds <- final_workflow |>
predict(new_data = GGGtest, type = "class")
View(nn_preds)
# Prep for Kaggle submission
kaggle_sub <- nn_preds %>%
bind_cols(., GGGtest) |> # Bind predictions to test data
rename(type = .pred_class) |> # Rename .pred_class to type for Kaggle submission
select(id, type) # Keep id and type variables
# Write out file
vroom_write(x = kaggle_sub, file = "./NNPreds.csv", delim = ",")
# Create recipe
GGG_recipe <- recipe(type ~ ., data = GGGtrain) |>
step_mutate_at(color, fn = factor) |>
step_dummy(color) |>
step_range(all_numeric_predictors(), min = 0, max = 1)
# Create neural network model
nn_mod <- mlp(hidden_units = tune(),
epochs = tune()) |>
set_engine("keras") |>
set_mode("classification")
# Create workflow
nn_workflow <- workflow() |>
add_model(nn_mod) |>
add_recipe(GGG_recipe)
# Grid of values to tune over
tuning_grid <- grid_regular(hidden_units(),
epochs(),
levels = 5)
# Split data for CV
folds <- vfold_cv(GGGtrain, v = 10, repeats = 1)
# Run CV
CV_results <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
# Create neural network model
nn_mod <- mlp(hidden_units = tune(),
epochs = 100) |>
set_engine("keras") |>
set_mode("classification")
# Create workflow
nn_workflow <- workflow() |>
add_model(nn_mod) |>
add_recipe(GGG_recipe)
# Grid of values to tune over
tuning_grid <- grid_regular(hidden_units(),
levels = 5)
# Split data for CV
folds <- vfold_cv(GGGtrain, v = 10, repeats = 1)
# Run CV
CV_results <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
library(tidymodels)
library(vroom)
# Read in data
GGGtrain <- vroom("train.csv")
GGGtest <- vroom("test.csv")
# Create recipe
GGG_recipe <- recipe(type ~ ., data = GGGtrain) |>
step_mutate_at(color, fn = factor) |>
step_dummy(color) |>
step_range(all_numeric_predictors(), min = 0, max = 1)
# Create neural network model
nn_mod <- mlp(hidden_units = tune(),
epochs = 100) |>
set_engine("keras") |>
set_mode("classification")
# Create workflow
nn_workflow <- workflow() |>
add_model(nn_mod) |>
add_recipe(GGG_recipe)
# Grid of values to tune over
tuning_grid <- grid_regular(hidden_units(),
levels = 5)
# Split data for CV
folds <- vfold_cv(GGGtrain, v = 10, repeats = 1)
# Run CV
CV_results <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "accuracy")
# Finalize workflow and fit
final_workflow <- nn_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Make predictions
nn_preds <- final_workflow |>
predict(new_data = GGGtest, type = "class")
# Prep for Kaggle submission
kaggle_sub <- nn_preds %>%
bind_cols(., GGGtest) |> # Bind predictions to test data
rename(type = .pred_class) |> # Rename .pred_class to type for Kaggle submission
select(id, type) # Keep id and type variables
# Write out file
vroom_write(x = kaggle_sub, file = "./NNPreds.csv", delim = ",")
# Create recipe
GGG_recipe <- recipe(type ~ ., data = GGGtrain) |>
step_mutate_at(color = as.factor(color)) |>
step_dummy(color) |>
step_range(all_numeric_predictors(), min = 0, max = 1)
# Create neural network model
nn_mod <- mlp(hidden_units = tune(),
epochs = 100) |>
set_engine("keras") |>
set_mode("classification")
# Create recipe
GGG_recipe <- recipe(type ~ ., data = GGGtrain) |>
step_mutate(color = as.factor(color)) |>
step_dummy(color) |>
step_range(all_numeric_predictors(), min = 0, max = 1)
# Create neural network model
nn_mod <- mlp(hidden_units = tune(),
epochs = 100) |>
set_engine("keras") |>
set_mode("classification")
# Create recipe
GGG_recipe <- recipe(type ~ ., data = GGGtrain) |>
step_mutate(color = as.factor(color)) |>
step_dummy(color) |>
step_range(all_numeric_predictors(), min = 0, max = 1)
# Create neural network model
nn_mod <- mlp(hidden_units = tune(),
epochs = 100) |>
set_engine("keras") |>
set_mode("classification")
# Create workflow
nn_workflow <- workflow() |>
add_model(nn_mod) |>
add_recipe(GGG_recipe)
# Grid of values to tune over
tuning_grid <- grid_regular(hidden_units(),
levels = 5)
# Split data for CV
folds <- vfold_cv(GGGtrain, v = 5, repeats = 1)
# Run CV
CV_results <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "f_meas")
# Finalize workflow and fit
final_workflow <- nn_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Make predictions
nn_preds <- final_workflow |>
predict(new_data = GGGtest, type = "class")
# Prep for Kaggle submission
kaggle_sub <- nn_preds %>%
bind_cols(., GGGtest) |> # Bind predictions to test data
rename(type = .pred_class) |> # Rename .pred_class to type for Kaggle submission
select(id, type) # Keep id and type variables
# Write out file
vroom_write(x = kaggle_sub, file = "./NNPreds.csv", delim = ",")
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "roc_auc")
# Finalize workflow and fit
final_workflow <- nn_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Make predictions
nn_preds <- final_workflow |>
predict(new_data = GGGtest, type = "class")
# Prep for Kaggle submission
kaggle_sub <- nn_preds %>%
bind_cols(., GGGtest) |> # Bind predictions to test data
rename(type = .pred_class) |> # Rename .pred_class to type for Kaggle submission
select(id, type) # Keep id and type variables
# Write out file
vroom_write(x = kaggle_sub, file = "./NNPreds.csv", delim = ",")
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "sens")
# Finalize workflow and fit
final_workflow <- nn_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Make predictions
nn_preds <- final_workflow |>
predict(new_data = GGGtest, type = "class")
# Prep for Kaggle submission
kaggle_sub <- nn_preds %>%
bind_cols(., GGGtest) |> # Bind predictions to test data
rename(type = .pred_class) |> # Rename .pred_class to type for Kaggle submission
select(id, type) # Keep id and type variables
# Write out file
vroom_write(x = kaggle_sub, file = "./NNPreds.csv", delim = ",")
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "recall")
# Finalize workflow and fit
final_workflow <- nn_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "precision")
# Finalize workflow and fit
final_workflow <- nn_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "roc_auc")
# Finalize workflow and fit
final_workflow <- nn_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Find best tuning parameters
best_tune <- CV_results |>
select_best(metric = "accuracy")
# Finalize workflow and fit
final_workflow <- nn_workflow |>
finalize_workflow(best_tune) |>
fit(data = GGGtrain)
# Make predictions
nn_preds <- final_workflow |>
predict(new_data = GGGtest, type = "class")
# Prep for Kaggle submission
kaggle_sub <- nn_preds %>%
bind_cols(., GGGtest) |> # Bind predictions to test data
rename(type = .pred_class) |> # Rename .pred_class to type for Kaggle submission
select(id, type) # Keep id and type variables
# Write out file
vroom_write(x = kaggle_sub, file = "./NNPreds.csv", delim = ",")
final_workflow |> collect_metrics() |>
filter(.metric == "accuracy") |>
ggplot(aes(x = hidden_units, y = mean)) + geom_line()
# Create plot with hidden objects on x-axis and mean on y-axis
nn_tune_grid <- grid_regular(hidden_units(range = c(1, 20)),
levels = 5)
tuned_nn <- nn_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
tuned_nn |> collect_metrics() |>
filter(.metric == "accuracy") |>
ggplot(aes(x = hiddenobjects, y = mean)) + geom_line()
tuned_nn |> collect_metrics() |>
filter(.metric == "accuracy") |>
ggplot(aes(x = hidden_objects, y = mean)) + geom_line()
tuned_nn |> collect_metrics() |>
filter(.metric == "accuracy") |>
ggplot(aes(x = hidden_objects(), y = mean)) + geom_line()
tuned_nn |> collect_metrics() |>
filter(.metric == "accuracy") |>
ggplot(aes(x = hidden_units, y = mean)) + geom_line()
tuned_nn <- nn_workflow |>
tune_grid(resamples = folds,
grid = nn_tune_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
tuned_nn |> collect_metrics() |>
filter(.metric == "accuracy") |>
ggplot(aes(x = hidden_units, y = mean)) + geom_line()
# Create plot with hidden objects on x-axis and mean on y-axis
nn_tune_grid <- grid_regular(hidden_units(),
levels = 5)
tuned_nn <- nn_workflow |>
tune_grid(resamples = folds,
grid = nn_tune_grid,
metrics = metric_set(accuracy, roc_auc, f_meas,
sens, recall, precision))
tuned_nn |> collect_metrics() |>
filter(.metric == "accuracy") |>
ggplot(aes(x = hidden_units, y = mean)) + geom_line()
